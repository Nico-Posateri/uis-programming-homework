<!DOCTYPE html>
<html lang="eng">

    <head>
        <title>Van Jacobson</title>
        <link rel="stylesheet" href="van-jacobson-project.css">
    </head>

    <body>
        <header>
            <nav>
                <a href="home.html">Home</a> |
                <a href="biography.html">Biography</a> |
                <a href="contributions.html">Contributions</a> |
                <a href="impact.html">Impact</a> |
                <a href="gallery.html">Gallery</a> |
                <a href="references.html">References</a>
            </nav>
        </header>
        
        <h1>Van Jacobson and TCP/IP</h1>

        <h2>Van Jacobson's Impact; Then and Now...</h2>
        <p>
            Of Jacobson's numerous contributions, his most significant was implementing the TCP compression algorithm that allowed the 1988-1989 traffic surge to<br>
            to merge easily. Van Jacobson's work is credited as giving greater insight into the particularities of network congestion.
        </p>
        <p>
            In 2002, the Institute of Electrical and Electronics Engineers (IEEE) bestowed Jacobson with the Koji Kobayashi Computers and Communications Award for<br>
            developing the congestion mitigation mechanisms that would allow for a “successful scaling of the Internet”.
        </p>
        <p>
            The Internet would go on to expand in user capacity, from ~10,000,000 in the early 90's to roughly 4 billion users and growing by 2024.
        </p>
        <p>
            Interestingly, in 2011, Van Jacobson was involved in talks regarding "bufferbloat", a late 2000's phenomenon. This refers to the way long packet queues<br>
            were being created by the now larger buffers of intermediate gateways such as routers and modems. Packets are queued for a long time during congestion<br>
            because the congestion prevention protocols of TCP are tricked by the higher speed/capacity RAM, where packets would have been dropped immediately<br>
            during periods of congestion on hardware with lower-capacity RAM, as was common when the protocol was initially implemented.
        </p>
        <p>
            Random Early Detection (RED) was as it sounds—a protocol intended to employ stricter discipline on queueing techniques in order to better avoid congestion.<br>
            Internet Service Providers (ISPs) were hesitant to implement early RED, however, due to technical inconsistencies.
        </p>
        <p>
            Van Jacobson began writing a paper to highlight the errors of RED and suggest implementations which would see it brought to its full potential, however<br>
            the paper initially went unpublished for some time. Between jobs, Jacobson lost track of the document and the necessary software for editing it.
        </p>
    </body>

    <footer>
        <p>
            Sources used for Contributions: <i>Official Biography: Van Jacobson</i>, <i>The lost Van Jacobson paper that could save the Internet</i>
        </p>
    </footer>

</html>
